# Travel Lotara - Opik Evaluation System

Comprehensive LLM-as-a-judge evaluation framework for the Travel Lotara AI agent system using Comet Opik.

## âœ¨ NEW: Automatic Inline Evaluation

**Every user request is now automatically evaluated and logged to Comet!** ğŸ¯

When your agent processes a request:
1. Agent generates response
2. **Automatic evaluation** runs (Gemini-powered LLM-as-judge)
3. **Metrics logged to Comet trace** with scores

View real-time evaluation in your Comet dashboard:
```
travel_lotara_root_agent/
â”œâ”€â”€ inspiration_agent
â”œâ”€â”€ planning_formatter_agent
â””â”€â”€ inline_evaluation â­ (evaluation scores appear here!)
    â”œâ”€â”€ hallucination: 0.95
    â”œâ”€â”€ relevance: 0.88  
    â”œâ”€â”€ safety: 1.00
    â””â”€â”€ quality: 0.82
```

**Enable/Disable:**
```bash
# Enabled by default
$env:ENABLE_INLINE_EVALUATION="true"

# To disable
$env:ENABLE_INLINE_EVALUATION="false"
```

## ğŸ¯ Quick Start

### 1. Setup (5 minutes)

```bash
# Install dependencies
pip install opik litellm google-generativeai

# Set API keys
$env:OPIK_API_KEY="your-opik-key"      # Get from https://www.comet.com/signup
$env:GOOGLE_API_KEY="your-gemini-key"  # Get from https://makersuite.google.com/app/apikey
```

### 2. Test Inline Evaluation (2 minutes)

```bash
# Run example showing automatic evaluation
python tests/example_inline_evaluation.py
```

### 3. Run Full Demo (5 minutes)

```bash
# Comprehensive demo of all metrics
python tests/demo_opik_evaluations.py
```

This demonstrates:
- âœ… Hallucination detection
- âœ… Answer relevance scoring
- âœ… RAG context quality (precision/recall)
- âœ… Safety/moderation checks
- âœ… Custom travel quality metrics

### 3. View Results

Go to https://www.comet.com/opik/projects to see your evaluation dashboard!

## ğŸ“ File Structure

```
src/travel_lotara/core/eval/
â”œâ”€â”€ opik_showcase.py           # ğŸŒŸ Main: Opik LLM-as-judge metrics (Gemini-powered)
â”œâ”€â”€ inline_evaluation.py       # â­ NEW: Automatic evaluation for each request
â”œâ”€â”€ opik_evaluators.py         # Custom Opik metric classes
â”œâ”€â”€ judges.py                  # LLM judge framework
â”œâ”€â”€ experiments.py             # A/B testing & experiments
â””â”€â”€ online_evals/              # Online evaluation tools

tests/
â”œâ”€â”€ example_inline_evaluation.py   # â­ NEW: Inline eval examples
â”œâ”€â”€ demo_opik_evaluations.py       # ğŸ¬ Full demo (START HERE!)
â”œâ”€â”€ evaluate_live_agent.py         # ğŸ”§ Evaluate real agent outputs
â”œâ”€â”€ eval_test_dataset.py           # ğŸ“Š Test cases & examples

docs/
â””â”€â”€ OPIK_EVALUATION_GUIDE.md       # ğŸ“– Complete guide
```

## ğŸš€ Usage Examples

### Example 1: Automatic Inline Evaluation (Recommended!)

```bash
# Just use your agent normally - evaluation happens automatically!
python tests/example_inline_evaluation.py
```

Your agent code automatically evaluates each response:
```python
# No changes needed! Evaluation is automatic via callbacks
agent = get_root_agent()
result = agent.run("Plan a trip to Paris")

# Evaluation runs automatically and logs to Comet trace
# Check Comet UI to see the scores!
```

### Example 2: Manual Evaluation

```bash
# Evaluate specific agent output
python tests/evaluate_live_agent.py --query "Plan a 5-day trip to Tokyo"
```

### Example 3: Full Demo

```bash
# Run all demonstrations
python tests/demo_opik_evaluations.py
```

### Example 3: Custom Evaluation

```python
from src.travel_lotara.core.eval.opik_showcase import (
    OpikMetricsShowcase,
    create_evaluation_sample
)

# Create evaluator
showcase = OpikMetricsShowcase(model="openai/gpt-4o-mini")

# Create sample
sample = create_evaluation_sample(
    sample_id="test_001",
    user_query="What's the best time to visit Bali?",
    agent_output="The bes (Gemini-Powered)

1. **Hallucination** - Detects false claims  
   `Hallucination(model="gemini/gemini-2.0-flash-exp")`  â­ Default

2. **Answer Relevance** - Checks if answer matches query  
   `AnswerRelevance(model="gemini/gemini-2.0-flash-exp")`

3. **Context Precision** - Evaluates RAG context quality  
   `ContextPrecision(model="gemini/gemini-2.0-flash-exp")`

4. **Context Recall** - Checks if all relevant info included  
   `ContextRecall(model="gemini/gemini-2.0-flash-exp")`

5. **Moderation** - Safety & policy compliance  
   `Moderation(model="gemini/gemini-2.0-flash-exp
2. **Answer Relevance** - Checks if answer matches query  
   `AnswerRelevance(model="openai/gpt-4o-mini")`

3. **Context Precision** - Evaluates RAG context quality  
   `ContextPrecision(model="openai/gpt-4o-mini")`

4. **Context Recall** - Checks if all relevant info included  
   `ContextRecall(model="openai/gpt-4o-mini")`

5. **Moderation** - Safety & policy compliance  
   `Moderation(model="openai/gpt-4o-mini")`

### Custom Metrics

6. **Travel Quality G-Eval** - Travel-specific quality assessment
7. **Agent Task Completion** - Did agent complete the task?
8. **Agent Tool Correctness** - Were right tools used?

## ğŸ“ Learn More

- **Full Guide**: [docs/OPIK_EVALUATION_GUIDE.md](docs/OPIK_EVALUATION_GUIDE.md)
- **Opik Docs**: https://www.comet.com/docs/opik/
- **Metrics Reference**: https://www.comet.com/docs/opik/evaluation/metrics/overview

## ğŸ”‘ Key Features

âœ… **Automatic Inline Evaluation** - Every request evaluated in real-time â­ NEW!  
âœ… **Gemini-Powered** - Fast, free LLM-as-judge (default model)  
âœ… **Pre-built Metrics** - Use Opik's proven LLM-as-judge metrics  
âœ… **Custom Metrics** - Travel-specific quality evaluations  
âœ… **Auto-Logging** - All results logged to Opik dashboard  
âœ… **Test Dataset** - Comprehensive test cases included  
âœ… **Live Evaluation** - Evaluate real agent outputs  
âœ… **A/B Testing** - Compare different prompts/models  
âœ… **Regression Testing** - Prevent quality degradation  
âœ… **Comet Integration** - Seamless tracing with your agent execution  

##NEW: See automatic inline evaluation
python tests/example_inline_evaluation.py

# Demo all metrics
python tests/demo_opik_evaluations.py

# Evaluate live query
python tests/evaluate_live_agent.py --query "YOUR_QUERY"

# Run test suite  
python tests/evaluate_live_agent.py --test-suite golden

# Use different judge model (Gemini is default)
python tests/evaluate_live_agent.py --query "Plan Paris trip" --model openai/gpt-4o-mini

# Use different judge model
python tests/evaluate_live_agent.py --query "Plan Paris trip" --model anthropic/claude-3-5-sonnet-20241022
```

## ğŸ› Troubleshooting

**"NoGOOGLE_API_KEY="your-gemini-key"  # For Gemini (default)
```

**"Import error"**
```bash
pip install opik litellm google-generativeai
```

**"Inline evaluation not working"**
```bash
# Check if enabled
$env:ENABLE_INLINE_EVALUATION="true"

# Check logs
python -c "from src.travel_lotara.core.eval.inline_evaluation import get_inline_evaluator; print(get_inline_evaluator().enabled)"
**"Import error"**
```bash
pip install opik litellm openai anthropic
```

## ğŸ“ˆ Score Interpretation

| Score | Meaning | Action |
|-------|---------|--------|
| 0.90-1.00 | Excellent âœ… | Deploy |
| 0.75-0.89 | Good âš ï¸ | Minor fixes |
| 0.60-0.74 | Acceptable âš ï¸ | Review |
| 0.00-0.59 | Poor âŒ | Major issues |

## ğŸ¤ Support

- ğŸ’¬ [Opik Discord](https://discord.gg/opik)
- ğŸ“§ support@comet.com
- ğŸ“š [Full Documentation](docs/OPIK_EVALUATION_GUIDE.md)

---

**Built for the Encode Club x Comet Hackathon** ğŸ†

Demonstrating best-in-class LLM evaluation using Opik's powerful metrics framework.
